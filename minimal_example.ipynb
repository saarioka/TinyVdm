{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe5ae39",
   "metadata": {},
   "source": [
    "# VdM analysis - a minimal example\n",
    "\n",
    "This is a brief introduction to the technicalities of the VdM method.  \n",
    "It is not a entirely accurate description of the VdM Framework, but it is capable of producing same results given equal conditions.  \n",
    "\n",
    "It is nicer to look at scans where there are only a few colliding bunches, as the fit plots are displayed in the notebook.\n",
    "\n",
    "When in doubt, restart the kernel and run the whole program again.\n",
    "\n",
    "You can find a pre-run notebook including output in minimal_example.html\n",
    "\n",
    "*Prequisites:* Install the needed packages with `python3 -m pip install -U -r requirements.txt`\n",
    "\n",
    "**Original author: Santeri Saariokari  \n",
    "Thanks: Nimmitha Karunarathna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f5bfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tables\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "from scipy import stats\n",
    "from iminuit import Minuit\n",
    "from iminuit.cost import LeastSquares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd312e",
   "metadata": {},
   "source": [
    "### Required \"arguments\"\n",
    "* File name\n",
    "* Luminometer name\n",
    "* Fit type\n",
    "* Flag to draw plots (T/F)\n",
    "* Flag to calibrate beam current (T/F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ffc30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = '/eos/cms/store/group/dpg_bril/comm_bril/vdmdata/2021/original/7525/7525_2110302352_2110310014.hd5'\n",
    "fit = 'SG'\n",
    "luminometer = 'pltlumizero'\n",
    "plots = True  \n",
    "calibrate_beam_current = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cee8103",
   "metadata": {},
   "source": [
    "### Define fit functions\n",
    "* Single Gaussian\n",
    "* Single Gaussian + constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8f461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sg(x, peak, mean, cap_sigma):\n",
    "    return peak*np.exp(-(x-mean)**2/(2*cap_sigma**2))\n",
    "\n",
    "def sg_const(x, peak, mean, cap_sigma, constant):\n",
    "    return sg(x, peak, mean, cap_sigma) + constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e0291",
   "metadata": {},
   "source": [
    "Each function needs a mapping from string parameter, and also a set of initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f4858",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FIT_FUNCTIONS = {\n",
    "    'SG':       {'handle': sg,       'initial_values': {'peak': 1e-4, 'mean': 0, 'cap_sigma': 0.3}},\n",
    "    'SGConst': {'handle': sg_const, 'initial_values': {'peak': 1e-4, 'mean': 0, 'cap_sigma': 0.3, 'constant': 0}}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d4909",
   "metadata": {},
   "source": [
    "General settings and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58d02c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use(hep.style.CMS)\n",
    "\n",
    "outpath = f'output/{Path(filename).stem}' # Save output to this folder\n",
    "Path(outpath).mkdir(parents=True, exist_ok=True) # Create output folder if not existing already"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e86679",
   "metadata": {},
   "source": [
    "**H5 contains several tables under \"/\" (root group). Following tables will be used.**\n",
    "* luminometer Ex: pltlumizero - *time, rate in each bunch*\n",
    "* beam - *time, beam energy, beam intensity per bunch (from FBCT and DCCT)*\n",
    "* vdmscan - *to get scan conditions: time, fill, run, lumiSec, nibble, ip, beam separation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14af6533",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = tables.open_file(filename, 'r')\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeb00ba",
   "metadata": {},
   "source": [
    "## Create a dataframe with general info about the scan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d94ce",
   "metadata": {},
   "source": [
    "**Using the tables \"vdmscan\" and \"beam\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41279d82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = f.root.vdmscan.colnames #Get colum names from the vdmscan table\n",
    "print(\"\\n\".join(cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a126f02",
   "metadata": {},
   "source": [
    "**Scan conditions are saved in table *vdmscan*  with timestamps. It is assumed that these are not going to change during a scan (at least the ones we care about)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024542f",
   "metadata": {},
   "source": [
    "Get first row of table \"vdmscan\" to save scan conditions that are constant through the scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee57e22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "general_info = pd.DataFrame([list(f.root.vdmscan[0])], columns=cols)\n",
    "general_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d936465",
   "metadata": {},
   "source": [
    "**Get the scanning points as a list.**\n",
    "\n",
    "IP is represented in a 8-bit binary number(Ex: 0b100000), which is read from the table as a decimal number(Ex: 32)\n",
    "\n",
    "Convert dec to binary list all scanning IPs (scan in IP n iff bit n == 1)\n",
    "\n",
    "Ex: 32 -> 0b100000 -> IP5\n",
    "\n",
    "Note the potentially confusing situtation: 2 means ATLAS, as 0b000001 = 2^1 = 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e526198a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "general_info['ip'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1251399",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "general_info['ip'] = general_info['ip'].apply(lambda ip: [i for i,b in enumerate(bin(ip)[::-1]) if b == '1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f1f2d",
   "metadata": {},
   "source": [
    "**Beam energy can be retrieved from the \"beam\" table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60bbe96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "general_info['energy'] = f.root.beam[0]['egev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12127021",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "general_info = general_info[['fillnum', 'runnum', 'timestampsec', 'energy', 'ip', 'bstar5', 'xingHmurad']]\n",
    "general_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527d49d",
   "metadata": {},
   "source": [
    "## Associate timestamps to pairs of scan planes and scan points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4950b9a",
   "metadata": {},
   "source": [
    "Get timestamps (timestampsec), beam seperation (sep) and scanning plane (nominal_sep_plane) when the *stat == ACQUIRING*\n",
    "\n",
    "* CROSSING == X Plane\n",
    "* SEPRARATION == Y Plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ae4d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scan = pd.DataFrame()\n",
    "\n",
    "scan['timestampsec'] = [r['timestampsec'] for r in f.root.vdmscan.where('stat == \"ACQUIRING\"')]\n",
    "scan['sep'] = [r['sep'] for r in f.root.vdmscan.where('stat == \"ACQUIRING\"')]\n",
    "scan['nominal_sep_plane'] = [r['nominal_sep_plane'].decode('utf-8') for r in f.root.vdmscan.where('stat == \"ACQUIRING\"')] # Decode is needed for values of type string\n",
    "scan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada213a",
   "metadata": {},
   "source": [
    "**Group data by plane and then by separation. For each separation list the minumum and maximum timestamp. This basically list the beginning and end of each scan step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126cf08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scan = scan.groupby(['nominal_sep_plane', 'sep']).agg(min_time=('timestampsec', np.min), max_time=('timestampsec', np.max)) # Get min and max for each plane - sep pair\n",
    "scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c258a88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scan.reset_index(inplace=True) # Cast <groupby> to normal dataframe\n",
    "print('\\nScanpoints and timestamps\\n', scan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f616494",
   "metadata": {},
   "source": [
    "## Collecting observable rates from the luminometer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a2e69",
   "metadata": {},
   "source": [
    "**The column 'collidable' in f.root.beam contains colliding bunches as an array of length 3564 (0-indexed).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20515539",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f.root.beam[0]['collidable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49ad82f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collidable = np.nonzero(f.root.beam[0]['collidable'])[0]  # np.nonzero returns 2 arrays where the first contains indices\n",
    "print(\"collidable\", collidable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b143bd",
   "metadata": {},
   "source": [
    "**bxconfig1 and bxconfig2 contain the filled bunches in each beam. Logical OR will give all filled bunches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef4e557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filled = np.nonzero(np.logical_or(f.root.beam[0]['bxconfig1'], f.root.beam[0]['bxconfig2']))[0] \n",
    "print(\"filled\", filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e65d3",
   "metadata": {},
   "source": [
    "### Go through each scan point and get rate and beam currect values for each colliding bunch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69f42ea",
   "metadata": {},
   "source": [
    "The column *bxraw* in f.root.pltlumizero contains rates of each bunch per integration time unit (NB4). It is needed to extract bxraw from the table based on the period of each seperation scan. This has to be done for each seperation for both planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c7e92a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f.root[luminometer][:]['bxraw'])\n",
    "print(f.root[luminometer][:]['bxraw'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff9d03",
   "metadata": {},
   "source": [
    "**For each plane & for each sep**\n",
    "1. Calculate \"period_of_scanpoint\". This is a query with scan begin and end times. This will be used to query the \"beam\" table.\n",
    "    \n",
    "    Ex: (timestampsec > 1635638276) & (timestampsec <= 1635638305)\n",
    "\n",
    "    **Do the following for the period_of_scanpoint**\n",
    "\n",
    "2. Get rates for colliding bunches from luminometer table and take the mean\n",
    "3. Calculate rate error using Standard Error of the Mean (SEM)\n",
    "4. Get bunch intensity for beam 1 and 2 from beam table. Columns \"bxintensity1\" and \"bxintensity2\" and take the mean\n",
    "5. Get beam intensities again using columns \"intensity1\" and \"intensity2\" and take the mean\n",
    "\n",
    "---\n",
    "\n",
    "**Why there is two separate sets of values for beam current:**  \n",
    "We have two systems to measure beam current. FBCT gives intensity per bunch, and DCCT gives the sum over all bunches. And we want per bunch. But DCCT is more accurate.\n",
    "So what is done, is bunch-by-bunch variation is taken from FBCT and normalized so that the sum equals DCCT value.\n",
    "This is the FBCT/DCCT calibration mentioned in the framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dd1faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "for p, plane in enumerate(scan.nominal_sep_plane.unique()):\n",
    "    for sep in scan.sep.unique():\n",
    "        new = pd.DataFrame()\n",
    "        new['bcid'] = collidable + 1 # From 0-indexed to 1-indexed\n",
    "        \n",
    "        # building the time query for the current separation \n",
    "        sep_start = scan.min_time[(scan.nominal_sep_plane == plane) & (scan.sep == sep)].item()\n",
    "        sep_end = scan.max_time[(scan.nominal_sep_plane == plane) & (scan.sep == sep)].item()\n",
    "        period_of_scanpoint = f'(timestampsec > {sep_start}) & (timestampsec <= {sep_end})'\n",
    "\n",
    "        # Only get rate for colliding bunches\n",
    "        r = np.array([r['bxraw'][collidable] for r in f.root[luminometer].where(period_of_scanpoint)]) \n",
    "        new['rate'] = r.mean(axis=0) # Mean over LS\n",
    "        new['rate_err'] = stats.sem(r, axis=0)\n",
    "        \n",
    "        new['fbct1'] = np.array([b['bxintensity1'][collidable] for b in f.root['beam'].where(period_of_scanpoint)]).mean(axis=0)\n",
    "        new['fbct2'] = np.array([b['bxintensity2'][collidable] for b in f.root['beam'].where(period_of_scanpoint)]).mean(axis=0)\n",
    "        \n",
    "        # DCCT is not per bunch, instead same value that contains the sum over BCIDs is repeated for all BCIDs\n",
    "        new['dcct1'] = np.array([b['intensity1'] for b in f.root['beam'].where(period_of_scanpoint)]).mean(axis=0)\n",
    "        new['dcct2'] = np.array([b['intensity2'] for b in f.root['beam'].where(period_of_scanpoint)]).mean(axis=0)\n",
    "        \n",
    "        # Additional quantities are needed for beam current calibration\n",
    "        if calibrate_beam_current:\n",
    "            fbct_filled1 = np.array([b['bxintensity1'][filled] for b in f.root['beam'].where(period_of_scanpoint)]).mean(axis=0).sum()\n",
    "            fbct_filled2 = np.array([b['bxintensity2'][filled] for b in f.root['beam'].where(period_of_scanpoint)]).mean(axis=0).sum()\n",
    "            new['fbct_to_dcct_beam1'] = fbct_filled1 / new['dcct1']\n",
    "            new['fbct_to_dcct_beam2'] = fbct_filled2 / new['dcct2']\n",
    "\n",
    "        new.insert(0, 'sep', sep) # Inserting constant as a column value will fill the column with the value\n",
    "        new.insert(0, 'plane', plane)\n",
    "        data = pd.concat([data, new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e589f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75720f",
   "metadata": {},
   "source": [
    "#### Normalize the rate by the product of beam currents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254795c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beam = data['fbct1'] * data['fbct2'] / 1e22  # dividing by 1e22 produces the desired units for sigvis in the end\n",
    "data['rate_normalised'] = data.rate / beam\n",
    "data['rate_normalised_err'] = data.rate_err / beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb8488",
   "metadata": {},
   "source": [
    "Calibrate beam current if specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0748aaee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if calibrate_beam_current:\n",
    "    calib = data.groupby('plane')[['fbct_to_dcct_beam1', 'fbct_to_dcct_beam2']].mean()\n",
    "    print('\\nFBCT to DCCT calibration coefficients\\n', calib)\n",
    "    \n",
    "    calib = calib.prod(axis=1) # Mean over LS, prod over beams\n",
    "    print(\"\\nProduct over beams\\n\",calib)\n",
    "    \n",
    "    for p in calib.index:\n",
    "        data.loc[data.plane == p, 'rate_normalised'] *= calib[calib.index == p].item()\n",
    "        data.loc[data.plane == p, 'rate_normalised_err'] *= calib[calib.index == p].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b8ab5",
   "metadata": {},
   "source": [
    "Add sensible error in case of 0 rate: max of error. The uncertainty would otherwise be 0, which is not true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2727722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['rate_normalised_err'].replace(0, data['rate_normalised_err'].max(), inplace=True)\n",
    "data.to_csv(f'{outpath}/{luminometer}_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d69a14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3f0d2",
   "metadata": {},
   "source": [
    "Make a fit for each BCID in both planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aace5bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit_results = pd.DataFrame()\n",
    "for p, plane in tqdm(enumerate(data.plane.unique())): # For each plane\n",
    "    for bcid in tqdm(collidable+1, leave=False): # For each BCID\n",
    "        data_x = scan[scan.nominal_sep_plane == plane]['sep']\n",
    "        data_y = data[(data.plane == plane) & (data.bcid == bcid)]['rate_normalised']\n",
    "        data_y_err = data[(data.plane == plane) & (data.bcid == bcid)]['rate_normalised_err']\n",
    "        \n",
    "        # Initialise minimiser with data and fit function of choice\n",
    "        least_squares = LeastSquares(data_x, data_y, data_y_err, FIT_FUNCTIONS[fit]['handle'])\n",
    "        m = Minuit(least_squares, **FIT_FUNCTIONS[fit]['initial_values']) # Initial values defined in \"FIT_FUNCTIONS\"\n",
    "        \n",
    "        m.migrad()  # Finds minimum of least_squares function\n",
    "        m.hesse()  # Accurately computes uncertainties\n",
    "        \n",
    "        print(m)\n",
    "        \n",
    "        new = pd.DataFrame([m.values], columns=m.parameters) # Store values to dataframe\n",
    "        \n",
    "        # Add suffix \"_err\" to errors\n",
    "        new = pd.concat([new, pd.DataFrame([m.errors], columns=m.parameters).add_suffix('_err')], axis=1)\n",
    "        \n",
    "        # Save fit status\n",
    "        new['valid'] =  m.valid\n",
    "        new['accurate'] = m.accurate\n",
    "        \n",
    "        new.insert(0, 'bcid', bcid)\n",
    "        new.insert(0, 'plane', plane)\n",
    "        fit_results = pd.concat([fit_results, new], ignore_index=True)\n",
    "        \n",
    "        # From here on just plotting\n",
    "        if plots:\n",
    "            # Initialise upper part: fit and data points\n",
    "            fig = plt.figure()\n",
    "            ax1 = fig.add_axes((.12,.3,.83,.65))\n",
    "\n",
    "            hep.cms.label(llabel=\"Preliminary\",\n",
    "                rlabel=fr\"Fill {general_info.fillnum[0]}, $\\sqrt{{s}}={general_info['energy'][0]*2/1000:.1f}$ TeV\", loc=1)\n",
    "\n",
    "            ax1.set_ylabel('$R/(N_1 N_2)$ [arb.]')\n",
    "            ax1.set_xticklabels([])\n",
    "            ax1.ticklabel_format(axis='y', style='sci', scilimits=(0,0), useMathText=True, useOffset=False)\n",
    "            ax1.minorticks_off()\n",
    "\n",
    "            # Initialise lower part: residuals\n",
    "            ax2 = fig.add_axes((.12,.1,.83,.2))\n",
    "            ax2.ticklabel_format(axis='y', style='plain', useOffset=False)\n",
    "            ax2.set_ylabel('Residual [$\\sigma$]',fontsize=20)\n",
    "            ax2.set_xlabel('$\\Delta$ [mm]')\n",
    "            ax2.minorticks_off()\n",
    "            \n",
    "            # Plot the data points\n",
    "            figure_items = []\n",
    "            figure_items.append(ax1.errorbar(data_x, data_y, data_y_err, fmt='ko'))\n",
    "            x_dense = np.linspace(np.min(data_x), np.max(data_x))\n",
    "            \n",
    "            # Plot the fit result\n",
    "            figure_items.append(ax1.plot(x_dense, FIT_FUNCTIONS[fit]['handle'](x_dense, *m.values), 'k'))\n",
    "\n",
    "            fit_info = [f'{plane}, BCID {bcid}', f'$\\\\chi^2$ / $n_\\\\mathrm{{dof}}$ = {m.fval:.1f} / {len(data_x) - m.nfit}']\n",
    "            for param, v, e in zip(m.parameters, m.values, m.errors):\n",
    "                fit_info.append(f'{param} = ${v:.3e} \\\\pm {e:.3e}$')\n",
    "\n",
    "            fit_info = [info.replace('cap_sigma', '$\\Sigma$') for info in fit_info]\n",
    "\n",
    "            figure_items.append(ax1.text(0.95, 0.95, '\\n'.join(fit_info), transform=ax1.transAxes, fontsize=14, fontweight='bold',\n",
    "                verticalalignment='top', horizontalalignment='right'))\n",
    "\n",
    "            residuals = (data_y.to_numpy() - FIT_FUNCTIONS[fit]['handle'](data_x, *m.values).to_numpy()) / data_y_err.to_numpy()\n",
    "            figure_items.append(ax2.scatter(data_x, residuals, c='k'))\n",
    "            \n",
    "            # Plot long \"zero-line\" without changing xlim\n",
    "            lim = list(plt.xlim())\n",
    "            figure_items.append(ax2.plot(lim, [0, 0], 'k:'))\n",
    "            plt.xlim(lim)\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "            # Only delete lines and fit results, leave general things\n",
    "            for item in figure_items:\n",
    "                if isinstance(item, list):\n",
    "                    item[0].remove()\n",
    "                else:\n",
    "                    item.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f125e5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_results.cap_sigma *= 1e3 # to µm\n",
    "fit_results.cap_sigma_err *= 1e3 # to µm\n",
    "fit_results.to_csv(f'{outpath}/{luminometer}_fit_results.csv', index=False)\n",
    "fit_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1476f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fr = fit_results.pivot(index='bcid', columns=['plane'], values=['cap_sigma', 'peak', 'cap_sigma_err', 'peak_err'])\n",
    "fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e028c7a",
   "metadata": {},
   "source": [
    " $\\sigma_{vis} = 2\\pi\\Sigma_x\\Sigma_y\\frac{peak_1+peak_2}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigvis = np.pi * fr.cap_sigma.prod(axis=1) * fr.peak.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5a9414",
   "metadata": {},
   "source": [
    "Propagation of uncertainty: $\\Delta(\\sigma_{vis}) = \\sigma_{vis}\\sqrt{\\frac{\\Delta(\\Sigma_x)^2}{\\Sigma_x^2} + \\frac{\\Delta(\\Sigma_y)^2}{\\Sigma_y^2} + \\frac{\\Delta(peak_x)^2 + \\Delta(peak_y)^2}{(peak_x+peak_y)^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219889a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigvis_err = (fr.cap_sigma_err**2 / fr.cap_sigma**2).sum(axis=1) + (fr.peak_err**2).sum(axis=1) / (fr.peak).sum(axis=1)**2\n",
    "sigvis_err = np.sqrt(sigvis_err) * sigvis\n",
    "lumi = pd.concat([sigvis, sigvis_err], axis=1)\n",
    "lumi.columns = ['sigvis', 'sigvis_err']\n",
    "lumi.to_csv(f'{outpath}/{luminometer}_lumi.csv')\n",
    "lumi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
